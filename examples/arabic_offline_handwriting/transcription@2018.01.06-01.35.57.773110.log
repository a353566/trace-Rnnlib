task = transcription

network:
task = transcription
13MultilayerNet
------------------------------
26 layers:
10InputLayer "input" 2D (++) size 1
10BlockLayer "input_block" 2D (++) size 12 source "input" block 3 4
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_0_0" 2D (--) inputSize 10 outputSize 2 source "input_block" 8 peeps
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_0_1" 2D (-+) inputSize 10 outputSize 2 source "input_block" 8 peeps
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_0_2" 2D (+-) inputSize 10 outputSize 2 source "input_block" 8 peeps
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_0_3" 2D (++) inputSize 10 outputSize 2 source "input_block" 8 peeps
10BlockLayer "hidden_0_0_block" 2D (++) size 24 source "hidden_0_0" block 3 4
10BlockLayer "hidden_0_1_block" 2D (++) size 24 source "hidden_0_1" block 3 4
10BlockLayer "hidden_0_2_block" 2D (++) size 24 source "hidden_0_2" block 3 4
10BlockLayer "hidden_0_3_block" 2D (++) size 24 source "hidden_0_3" block 3 4
11NeuronLayerI4TanhE "subsample_0" 2D (++) size 6 source "hidden_0_0_block"
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_1_0" 2D (--) inputSize 50 outputSize 10 source "subsample_0" 40 peeps
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_1_1" 2D (-+) inputSize 50 outputSize 10 source "subsample_0" 40 peeps
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_1_2" 2D (+-) inputSize 50 outputSize 10 source "subsample_0" 40 peeps
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_1_3" 2D (++) inputSize 50 outputSize 10 source "subsample_0" 40 peeps
10BlockLayer "hidden_1_0_block" 2D (++) size 80 source "hidden_1_0" block 2 4
10BlockLayer "hidden_1_1_block" 2D (++) size 80 source "hidden_1_1" block 2 4
10BlockLayer "hidden_1_2_block" 2D (++) size 80 source "hidden_1_2" block 2 4
10BlockLayer "hidden_1_3_block" 2D (++) size 80 source "hidden_1_3" block 2 4
11NeuronLayerI4TanhE "subsample_1" 2D (++) size 20 source "hidden_1_0_block"
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_2_0" 2D (--) inputSize 250 outputSize 50 source "subsample_1" 200 peeps
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_2_1" 2D (-+) inputSize 250 outputSize 50 source "subsample_1" 200 peeps
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_2_2" 2D (+-) inputSize 250 outputSize 50 source "subsample_1" 200 peeps
(R) 9LstmLayerI4TanhS0_8LogisticE "hidden_2_3" 2D (++) inputSize 250 outputSize 50 source "subsample_1" 200 peeps
13CollapseLayer "output_collapse" 1D (+) size 121 source "hidden_2_3"
18TranscriptionLayer "output" 1D (+) size 121 source "output_collapse"
------------------------------
62 connections:
"bias_to_hidden_0_0" (10 wts)
"hidden_0_0_to_hidden_0_0_delay_1_0" (20 wts)
"hidden_0_0_to_hidden_0_0_delay_0_1" (20 wts)
"input_block_to_hidden_0_0" (120 wts)
"bias_to_hidden_0_1" (10 wts)
"hidden_0_1_to_hidden_0_1_delay_1_0" (20 wts)
"hidden_0_1_to_hidden_0_1_delay_0_-1" (20 wts)
"input_block_to_hidden_0_1" (120 wts)
"bias_to_hidden_0_2" (10 wts)
"hidden_0_2_to_hidden_0_2_delay_-1_0" (20 wts)
"hidden_0_2_to_hidden_0_2_delay_0_1" (20 wts)
"input_block_to_hidden_0_2" (120 wts)
"hidden_0_0_block_to_subsample_0" (144 wts)
"hidden_0_1_block_to_subsample_0" (144 wts)
"hidden_0_2_block_to_subsample_0" (144 wts)
"hidden_0_3_block_to_subsample_0" (144 wts)
"bias_to_hidden_1_0" (50 wts)
"hidden_1_0_to_hidden_1_0_delay_1_0" (500 wts)
"hidden_1_0_to_hidden_1_0_delay_0_1" (500 wts)
"subsample_0_to_hidden_1_0" (300 wts)
"bias_to_hidden_1_1" (50 wts)
"hidden_1_1_to_hidden_1_1_delay_1_0" (500 wts)
"hidden_1_1_to_hidden_1_1_delay_0_-1" (500 wts)
"subsample_0_to_hidden_1_1" (300 wts)
"bias_to_hidden_1_2" (50 wts)
"hidden_1_2_to_hidden_1_2_delay_-1_0" (500 wts)
"hidden_1_2_to_hidden_1_2_delay_0_1" (500 wts)
"subsample_0_to_hidden_1_2" (300 wts)
"bias_to_hidden_1_3" (50 wts)
"hidden_1_3_to_hidden_1_3_delay_-1_0" (500 wts)
"hidden_1_3_to_hidden_1_3_delay_0_-1" (500 wts)
"subsample_0_to_hidden_1_3" (300 wts)
"hidden_1_0_block_to_subsample_1" (1600 wts)
"hidden_1_1_block_to_subsample_1" (1600 wts)
"hidden_1_2_block_to_subsample_1" (1600 wts)
"hidden_1_3_block_to_subsample_1" (1600 wts)
"bias_to_hidden_2_0" (250 wts)
"hidden_2_0_to_hidden_2_0_delay_1_0" (12500 wts)
"hidden_2_0_to_hidden_2_0_delay_0_1" (12500 wts)
"subsample_1_to_hidden_2_0" (5000 wts)
"bias_to_hidden_2_1" (250 wts)
"hidden_2_1_to_hidden_2_1_delay_1_0" (12500 wts)
"hidden_2_1_to_hidden_2_1_delay_0_-1" (12500 wts)
"subsample_1_to_hidden_2_1" (5000 wts)
"bias_to_hidden_2_2" (250 wts)
"hidden_2_2_to_hidden_2_2_delay_-1_0" (12500 wts)
"hidden_2_2_to_hidden_2_2_delay_0_1" (12500 wts)
"subsample_1_to_hidden_2_2" (5000 wts)
"bias_to_hidden_2_3" (250 wts)
"hidden_2_3_to_hidden_2_3_delay_-1_0" (12500 wts)
"hidden_2_3_to_hidden_2_3_delay_0_-1" (12500 wts)
"subsample_1_to_hidden_2_3" (5000 wts)
"bias_to_output" (121 wts)
"output_collapse_to_output" (copy)
"hidden_2_0_to_output_collapse" (6050 wts)
"hidden_2_1_to_output_collapse" (6050 wts)
"hidden_2_2_to_output_collapse" (6050 wts)
"hidden_2_3_to_output_collapse" (6050 wts)
"bias_to_hidden_0_3" (10 wts)
"hidden_0_3_to_hidden_0_3_delay_-1_0" (20 wts)
"hidden_0_3_to_hidden_0_3_delay_0_-1" (20 wts)
"input_block_to_hidden_0_3" (120 wts)
------------------------------
bidirectional = true true
symmetry = false false
inputBlock = 3 4
159369 weights

setting random seed to 2520265262

159369 uninitialised weights randomised uniformly in [-0.1,0.1]
trainer:
epoch = 0
savename = transcription@2018.01.06-01.35.57.773110
batchLearn = false
seqsPerWeightUpdate = 1
maxTestsNoBest = 20

training data:
numSequences = 20
numTimesteps = 756020
avg timesteps/seq = 37801
1 filenames
arabic_offline.nc
inputSize = 1
outputSize = 120
numDims = 2
task = transcription
shuffled = true
120 target labels
0 aaElaB (0 = 0%)
1 0A (0 = 0%)
2 aaElaM (1 = 0.632911%)
3 1A (0 = 0%)
4 baA (0 = 0%)
5 2A (0 = 0%)
6 baB (6 = 3.79747%)
7 6A (0 = 0%)
8 baE (0 = 0%)
9 7A (0 = 0%)
10 baM (3 = 1.89873%)
11 8A (0 = 0%)
12 teA (4 = 2.53165%)
13 9A (0 = 0%)
14 taA (0 = 0%)
15 hhA (1 = 0.632911%)
16 taB (1 = 0.632911%)
17 amA (0 = 0%)
18 teE (8 = 5.06329%)
19 aeA (1 = 0.632911%)
20 taE (0 = 0%)
21 ahA (0 = 0%)
22 taM (0 = 0%)
23 aeElaB (0 = 0%)
24 thA (0 = 0%)
25 ahElaB (0 = 0%)
26 thB (0 = 0%)
27 alM (1 = 0.632911%)
28 thM (0 = 0%)
29 aaA (17 = 10.7595%)
30 jaA (0 = 0%)
31 aaE (6 = 3.79747%)
32 jaB (0 = 0%)
33 jaE (1 = 0.632911%)
34 zaM (1 = 0.632911%)
35 jaM (3 = 1.89873%)
36 ayA (0 = 0%)
37 jaMlaB (0 = 0%)
38 ayB (1 = 0.632911%)
39 haA (1 = 0.632911%)
40 ayE (0 = 0%)
41 haB (1 = 0.632911%)
42 ayM (0 = 0%)
43 haE (0 = 0%)
44 ghB (0 = 0%)
45 haM (1 = 0.632911%)
46 ghM (0 = 0%)
47 haMlaB (1 = 0.632911%)
48 faA (1 = 0.632911%)
49 haMmaMlaB (0 = 0%)
50 faB (0 = 0%)
51 haMnaB (0 = 0%)
52 faE (1 = 0.632911%)
53 khA (0 = 0%)
54 faM (2 = 1.26582%)
55 khB (0 = 0%)
56 kaA (1 = 0.632911%)
57 khE (0 = 0%)
58 kaB (3 = 1.89873%)
59 khM (1 = 0.632911%)
60 kaE (0 = 0%)
61 khMlaB (1 = 0.632911%)
62 kaM (1 = 0.632911%)
63 daA (1 = 0.632911%)
64 keB (0 = 0%)
65 daE (3 = 1.89873%)
66 keE (0 = 0%)
67 dhA (0 = 0%)
68 keM (0 = 0%)
69 dhE (0 = 0%)
70 laA (2 = 1.26582%)
71 raA (4 = 2.53165%)
72 laB (13 = 8.22785%)
73 raE (8 = 5.06329%)
74 laE (0 = 0%)
75 zaA (1 = 0.632911%)
76 laM (3 = 1.89873%)
77 zaE (3 = 1.89873%)
78 maA (0 = 0%)
79 seA (0 = 0%)
80 maB (4 = 2.53165%)
81 seB (3 = 1.89873%)
82 maE (1 = 0.632911%)
83 seE (1 = 0.632911%)
84 maM (4 = 2.53165%)
85 seM (0 = 0%)
86 maMlaB (0 = 0%)
87 shA (0 = 0%)
88 naA (1 = 0.632911%)
89 shB (1 = 0.632911%)
90 naB (1 = 0.632911%)
91 shE (0 = 0%)
92 naE (0 = 0%)
93 shM (0 = 0%)
94 naM (4 = 2.53165%)
95 saA (0 = 0%)
96 heA (0 = 0%)
97 saB (2 = 1.26582%)
98 heB (1 = 0.632911%)
99 saE (1 = 0.632911%)
100 heE (0 = 0%)
101 saM (1 = 0.632911%)
102 heM (0 = 0%)
103 deA (0 = 0%)
104 waA (1 = 0.632911%)
105 deB (0 = 0%)
106 waE (10 = 6.32911%)
107 deE (1 = 0.632911%)
108 eeA (0 = 0%)
109 deM (0 = 0%)
110 yaA (1 = 0.632911%)
111 toA (0 = 0%)
112 yaB (5 = 3.16456%)
113 toB (1 = 0.632911%)
114 eeE (0 = 0%)
115 toE (0 = 0%)
116 yaE (0 = 0%)
117 toM (1 = 0.632911%)
118 yaM (5 = 3.16456%)
119 zaB (0 = 0%)
totalTargetStringLength = 158

------------------------------
steepest descent
learnRate = 0.0001
momentum = 0.9
------------------------------

autosave filename transcription@2018.01.06-01.35.57.773110.last.save
best save filename root transcription@2018.01.06-01.35.57.773110.best

training...

epoch 0 took 0.781103 seconds (0.0390551 secs/seq, 967888 its/sec, 709.556 MwtOps/sec)
------------------------------
train errors (running):
ctcError 80.3054
deletions 83.5443%
insertions 0%
labelError 100%
seqError 100%
substitutions 16.4557%
------------------------------
saving to transcription@2018.01.06-01.35.57.773110.last.save
best network (ctcError)
saving to transcription@2018.01.06-01.35.57.773110.best_ctcError.save
best network (labelError)
saving to transcription@2018.01.06-01.35.57.773110.best_labelError.save

epoch 1 took 0.765276 seconds (0.0382638 secs/seq, 987905 its/sec, 724.23 MwtOps/sec)
------------------------------
train errors (running):
ctcError 65.7059
deletions 100%
insertions 0%
labelError 100%
seqError 100%
substitutions 0%
------------------------------
saving to transcription@2018.01.06-01.35.57.773110.last.save
best network (ctcError)
saving to transcription@2018.01.06-01.35.57.773110.best_ctcError.save
